%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[a4, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{multicol}
\usepackage{tcolorbox}
\usepackage{cuted,tcolorbox,lipsum}
\usepackage{xcolor}
\usepackage{hyperref}

\title{\LARGE \bf
Introduction to Machine Learning (SS 2025)\\ Programming Project
\vspace{-3em}
}


\begin{document}


\maketitle
\vspace{-3em}
\thispagestyle{empty}
\pagestyle{empty}

\begin{strip}
\begin{tcolorbox}[
size=tight,
colback=white,
boxrule=0.2mm,
left=3mm,right=3mm, top=3mm, bottom=1mm
]
{\begin{multicols}{2}% replace 3 with 2 for 2 authors.

\textbf{Author 1}\\
Last name: Gajdos\\
First name: Dominik\\
Matrikel Nr.:\\  % Enter Matrikel number

\columnbreak

\textbf{Author 2}\\
Last name: Wintner\\
First name: Patrick\\
Matrikel Nr.: 12143491\\

\columnbreak

% only four three person team
% \textbf{Author 3}       \\
% Last name:              \\  % Enter first name
% First name:             \\  % Enter first name
% Matrikel Nr.:               \\  % Enter Matrikel number

\end{multicols}}
\end{tcolorbox}
\end{strip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


{\color{blue}
  \noindent This template outlines the sections that your report must 
  contain. Inside each section, we provide pointers to what you should
  write about in that section (in blue text).  \linebreak

\noindent \textbf{Please remove all the text in blue in your report!
  Your report should be 2 pages for regular teams (excluding references!)
  and 3 pages for the three person team.}  }

\section{Introduction}
\label{sec:intro}

{\color{blue}

\begin{itemize}
	\item What is the nature of your task (regression/classification)? Is it about classifying types of birds, or deciding the number of cookies an employee receives?
	\item Describe the dataset (number of features, number of instances, types of features, missing data, data imbalances, or any other relevant information).
\end{itemize}
}

This report is covers the detection of fraudulent transactions using different binary classifiers. The dataset has 30 features and contains more than 227,800 entries belonging either to class 0 (not fraudulent) or 1 (fraudulent). The dataset is extremely imbalanced; roughly 99.8\% of all transactions are not fraudulent. All features are numerical.
\section{Implementation / ML Process}
\label{sec:methods}

{\color{blue}

\begin{itemize}
	\item Did you need to pre-process the dataset (e.g. augmenting data points, extracting features, reducing the dimensionality, etc.)? If so, describe how you did this.
	\item Specify the method (e.g. linear regression, or neural network, etc.). You do not have to describe the algorithm in detail, but rather the algorithm family and the properties of the algorithm within that family, e.g. which distance functions for a decision tree, what architecture (layers and activations) for a neural network, etc. 
	\item State (in 2-5 lines) what makes the algorithm you chose suitable for this problem. What are the reasons for choosing your ML method over others?
    \item If you used a method that was not covered in the VO, describe how it is different from the closest method described in the VO.
	\item How did you choose hyperparameters (other design choices) and what are the values of the hyperparameters you chose for your final model? How did you make sure that the choice of hyperparameters works well?
\end{itemize}
}
We decided to implement logistic regression and a random forest for this task. Both methods are binary classifiers and thus appropriate for this problem. Furthermore, random forests are recommended by Baeldung for fraud detection\cite{baeldung}. Each model has been implemented by one author, which has resulted in slightly different approaches for each classifier.

\subsection{Data Logistic Regression}

\subsection{Random Forest}
The implementations of principal component analysis and random forests of scikit-learn are used.
\subsubsection{Data-Preprocessing}
\label{subsubsection:data-processing}
The data is split into a training and test set, which contain 75\% and 25\% of all samples respectively. The dimensionality of the input data is reduced by using principal component analysis. The implementation of scikit-learn uses singular value decomposition without scaling the input data beforehand \cite{sl.pca}. The number of dimensions kept is determined via randomized search cross validation (see \ref{subsubsection:hyperparameters}); the other parameters are kept to their default values.
\subsubsection{Classifier Properties}
\label{subsubsection:classifier-properties}
Each tree in the ensemble is built from a sample drawn with replacement and using the best split strategy. The final prediction is the average of the probabilistic predictions of the individual trees (instead of using a voting mechanism). The most important hyperparameters according to the scikit-learn documentation are the number of trees in a forest and the number of features used to determine the best split \cite{sl.rf}. Those are determined via randomized search cross validation (see \ref{subsubsection:hyperparameters}), as is the criterion used to measure the impurity of a split. The other parameters are kept to their default values, therefore there is no maximal tree depth, no maximal number of leaf nodes and classes are not weighted.
\subsubsection{Hyperparameters}
\label{subsubsection:hyperparameters}
The most important hyperparameters (for search ranges and final choices see table \ref{tab:param.rf}) are determined by using the implementation of randomized search cross validation provided by scikit-learn. The randomized search cross validation is given a pipeline consisting of a PCA-Object and a random forest classifier. Five-fold cross-validation is used as cross-validation scheme and the ROC AUC score is used for performance measurement. Ten different parameter settings are sampled. The pipeline is refitted with the best parameters found.

K-fold cross validation works by splitting the training set into k smaller sets, training the model on k-1 sets and using the last set for validation. This is done k times so that each set is used once for validation. The performance is then computed by taking the average. \cite{sl.cv}

The implementation of randomized Cross validation search by scikit-learn optimizes parameters by sampling a number of parameter settings of a given parameter space, performing cross validation for the sampled setting, and keeping the parameters that resulted in the best score. Using a pipeline allows several steps (e.g. data-preprocessing followed by a classifier) to be cross-validated together. \cite{sl.rcv}

\begin{table*}[hbt]
	\begin{tabular}{c|c|c|c|c}
		& $n\_components$ & $n\_estimators$ & $max\_features$ & $criterion$\\
		\hline range & $\{1, 7, 13,\dots , 25\}$ & $\{10, 11,\dots, 100\}$ & $[0,1]$ & $\{gini, entropy, log\_loss\}$\\
		\hline choice & 19 & 47 & 0.225 & log\_loss
	\end{tabular}
	\caption{Hyperparameter selection random forest}
	\label{tab:param.rf}
\end{table*}

\section{Results}
\label{sec:results}

{\color{blue}

\begin{itemize}
	\item Describe the performance of your model (in terms of the metrics for your dataset) on the training and validation sets with the help of plots or/and tables.
	\item You must provide at least two separate visualizations
          (plot or tables) of different things, i.e. donâ€™t use a table
          and a bar plot of the same metrics. At least three
           visualizations are required for the 3 person team.
\end{itemize}
}

Both classifiers achieve acceptable results on the validation set, which can be seen in table \ref{tab:results}.

\begin{table*}[hbt]
	\begin{tabular}{c|c|c|c|c}
		classifier & ROC AUC Score & Accuracy score & Precision score & Recall score\\
		\hline logistic regression & & & &\\
		\hline random forest & 91.6\% & 99.97\% & 97.4\% & 83.1\%
	\end{tabular}
	\caption{Hyperparameter selection random forest}
	\label{tab:results}
\end{table*}

\section{Discussion}
\label{sec:discuss}

{\color{blue}
\begin{itemize}
	\item Analyze the results presented in the report (comment on what contributed to the good or bad results). If your method does not work well, try to analyze why this is the case.
	\item Describe very briefly what you tried but did not keep for your final implementation (e.g. things you tried but that did not work, discarded ideas, etc.).
	\item How could you try to improve your results? What else would you want to try?

\end{itemize}
}
\subsection{Logistic Regression}
\subsection{Random Forest}
The parameters determined by randomized cross-validation search lead to slightly better results than the default parameters. It hovever is computionally way more expensive.

The most obvious way to improve the results is to sample more parameter settings during randomized cross-validation search and by using tighter bounds for the spaces from wich the parameters are sampled. Another option to likely increase the performance is to set the parameter \emph{class\_weight} of the random forest classifier to 'balanced\_subsample' to take the imbalance of the dataset into account.

Surprisingly, the effect on performance of using principal component analysis is negligible. This is probably because the parameter max\_features, which determines the number of features used for determining the best split has a somewhat similar effect (reducing the number of relevant features). Interestingly, the chosen values for n\_components and for max\_features result in a number of relevant features, which is near default value of max\_features=$\sqrt{n\_features}$ without PCA: $n\_components*max\_features = 19*0.225 \approx 4.3$ and $max\_features\_default=\sqrt{n\_features}=\sqrt{30}\approx 5.5$.

\section{Conclusion}
\label{sec:con}

{\color{blue}

  \begin{itemize}
  \item Finally, describe the test-set performance you achieved. Do not
    optimize your method based on the test set performance!
  \item Write a 5-10 line paragraph describing the main takeaway of your project.
  \end{itemize}

}

\begin{thebibliography}{xxxx}
	\bibitem{baeldung} \url{https://www.baeldung.com/cs/random-forest-vs-extremely-randomized-trees} [online; last access on 02.07.2025]
	\bibitem{sl.pca} \url{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html} [online; last access on 02.07.2025]
	\bibitem{sl.rf} \url{https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles} [online; last access on 02.07.2025]
	\bibitem{sl.cv} \url{https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation} [online; last access on 03.07.2025]
	\bibitem{sl.rcv} \url{https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search}
\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}
